
## **Paragraphs you can drop into the article**

  

**What we are simulating**

  

In the OSWE labs, a time-based blind SQLi probe works because the injected payload reaches the database and causes the database to stall when a condition is true. The client never sleeps. The client just waits for the HTTP response, measures the elapsed time, and treats “slow” as true and “fast” as false. That distinction matters because it’s easy to accidentally build a demo where the timing signal comes from the wrong place. If the delay happens in the client, the oracle is fake and the extractor is just timing its own code, not the target.

  

**Why I’m not using a real database here**

  

For this article, I’m using a small Python server that simulates the same control-flow you get with a real time-based SQL injection. The server accepts a request that contains a predicate like “is the character at position N greater than X” and it deliberately delays the HTTP response only when that predicate is true. That delay represents what would normally be SLEEP(3) (MySQL) or pg_sleep(3) (Postgres) executing inside the database. The goal is to make the benchmark honest while keeping the setup minimal: the client still has to infer bits of a secret from response timing, and the only thing we changed is the implementation of the predicate engine.

  

**Why this helps when comparing linear, binary, and async**

  

This approach also makes it easier to compare strategies fairly. Linear extraction and binary search both use the same oracle and the same network path. The only differences are the number of probes required and how aggressively we schedule requests. Async does not make the oracle “more true”, it just stops us from idling between I/O operations. When the oracle is implemented correctly, async and async-binary win because they reduce wasted waiting. When the oracle is implemented incorrectly, async tends to fail faster and more obviously, which is useful in itself: it exposes when you are measuring your own client instead of the target.

---

## **A short “box” note you can add (optional)**

  

**Important note about where the sleep happens**

In real time-based blind SQLi, the sleep is executed by the database (because the injected payload is evaluated by the SQL engine). In this article, the sleep is executed by the server to simulate that same “request blocks only when predicate is true” behavior. The client does not sleep in either case.

---

## **What the simulator should support (so the text is truthful)**

  

To match your goals (linear vs binary + async variants, token rotates after a round, summary), the simulator should:

- Store token server-side (in memory is fine)
    
- Expose a “predicate” endpoint that takes pos and either:
    
    - equality probe (token[pos] == c) for linear brute force, and/or
        
    - comparison probe (ord(token[pos]) > mid) for binary search
        
    
- Delay the HTTP response by SLEEP_SECONDS only when predicate is true
    
- Provide a /done endpoint so the client can tell the server “linear done” then “async done”, and rotate after 2 completions
    
- Provide a /peek endpoint gated behind a flag for debugging (disabled by default in the article)
    

  

This keeps the “database sleep” concept intact while making the demo setup trivial.

---

**Q1:** Do you want the oracle to be _equality-based_ (linear) and _greater-than-based_ (binary), or do you want to teach binary search using equality only (which is possible but awkward)?

**Q2:** Do you want the token charset to be strictly [A-Za-z0-9] for readability, or printable symbols too (more realistic but noisier)?

**Q3:** Should the server enforce rate-limits or jitter to show how real targets degrade async performance, or keep it deterministic for first-pass clarity?